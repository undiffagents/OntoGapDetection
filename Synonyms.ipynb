{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "#Create Graph\n",
    "testGraph = networkx.MultiDiGraph()\n",
    "testGraph.add_node('UAV', value = 'UAV')\n",
    "testGraph.add_node('UAVAttributeType', value = 'course|speed|altitude|flaps|gear|fuel')\n",
    "testGraph.add_node('UAVAttributeValue', value = '<number>|unknown')\n",
    "testGraph.add_node('UAVAttributeStatus', value = 'up|down|unknown')\n",
    "testGraph.add_node('UAVAttributeAlarm', value = 'ok|warning|alarm')\n",
    "testGraph.add_node('UAVAttributeStable', value = 'true|false|unknown')\n",
    "testGraph.add_node('UAVAttributeChange', value = 'increase|decrease|stop')\n",
    "testGraph.add_node('UAVAttributeAction', value = 'move|climb|descend|speed_up|slow_down')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeType', value = 'UAVhasType')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeValue', value = 'UAVhasValue')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeStatus', value = 'UAVhasStatus')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeAlarm', value = 'UAVhasAlarm')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeStable', value = 'UAVhasStable')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeChange', value = 'UAVhasChange')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeAction', value = 'UAVhasAction')\n",
    "\n",
    "testGraph.add_node('Restriction', value='Restriction')\n",
    "testGraph.add_node('RestrictionUpper', value='<number>|none|unknown')\n",
    "testGraph.add_node('RestrictionLower', value='<number>|none|unknown')\n",
    "testGraph.add_node('RestrictionType', value='altitude|speed|photo')\n",
    "testGraph.add_edge('Restriction', 'RestrictionUpper', value='RestrictionHasUpper')\n",
    "testGraph.add_edge('Restriction', 'RestrictionLower', value='RestrictionHasLower')\n",
    "testGraph.add_edge('Restriction', 'RestrictionType', value='RestrictionHasType')\n",
    "\n",
    "testGraph.add_edge('UAV', 'Restriction', value='hasRestriction')\n",
    "\n",
    "#Draw Graph\n",
    "networkx.draw(testGraph, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNyms(anchorWords, checkWordList):\n",
    "    #Iterate through all words to check\n",
    "    synonyms = []\n",
    "    hypernyms = []\n",
    "    hyponyms = []\n",
    "    deriv = []\n",
    "    for currentWord in checkWordList:\n",
    "        synonyms.clear()\n",
    "        hypernyms.clear()\n",
    "        hyponyms.clear()\n",
    "        deriv.clear()\n",
    "        print(currentWord)\n",
    "        #print(currentWord)\n",
    "        #print(checkWordList[currentWord])\n",
    "        #Get synsets of current word to check\n",
    "        testWord = wordnet.synsets(currentWord)\n",
    "        #for each synset (meaning)\n",
    "        for syn in testWord:\n",
    "            #Get Hypernyms\n",
    "            if(len(syn.hypernyms()) > 0):\n",
    "                currentHypernyms = syn.hypernyms()\n",
    "                for hyperSyn in currentHypernyms:\n",
    "                    for lemma in hyperSyn.lemmas():\n",
    "                        #if(lemma.name() != currentWord):\n",
    "                        hypernyms.append(lemma.name())\n",
    "                    #hypernyms.append(hyperSyn.lemma_names())\n",
    "            #Get Hyponyms\n",
    "            if(len(syn.hyponyms()) > 0):\n",
    "                currentHyponyms = syn.hyponyms()\n",
    "                for hypoSyn in currentHyponyms:\n",
    "                    for lemma in hypoSyn.lemmas():\n",
    "                        #if(lemma.name() != currentWord):\n",
    "                        hyponyms.append(lemma.name())\n",
    "                    #hypernyms.append(hyperSyn.lemma_names())\n",
    "            #Get direct synonyms\n",
    "            for lemma in syn.lemmas():\n",
    "                #if(lemma.name() != currentWord):\n",
    "                synonyms.append(lemma.name())\n",
    "                #Get derivationally related forms\n",
    "                for derivForm in lemma.derivationally_related_forms():\n",
    "                    if(derivForm.name() not in deriv):\n",
    "                        deriv.append(derivForm.name())\n",
    "        #print(\"SYNONYMS: \")\n",
    "        #print(set(synonyms))\n",
    "        #print('\\n HYPERNYMS:')\n",
    "        #print(set(hypernyms))\n",
    "        #print('\\n HYPONYMS:')\n",
    "        #print(set(hyponyms))\n",
    "        #print('\\n DERIVATIONALLY RELATED FORMS:')\n",
    "        #print(set(deriv))\n",
    "        #Check if any target words found in syno/hyper/hypo lists\n",
    "        #If target word is found, increase the number of times found in the dictionary.\n",
    "        for targetWord in anchorWords:\n",
    "            if targetWord in set(synonyms):\n",
    "                anchorWords[targetWord].append(currentWord)\n",
    "            elif targetWord in set(hypernyms):\n",
    "                anchorWords[targetWord].append(currentWord)\n",
    "            elif targetWord in set(hyponyms):\n",
    "                anchorWords[targetWord].append(currentWord)\n",
    "            elif targetWord in set(deriv):\n",
    "                anchorWords[targetWord].append(currentWord)\n",
    "    return (synonyms, hypernyms, hyponyms, deriv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAnchors(wordToFind):\n",
    "    #Define vars\n",
    "    anchorNodeValues = []\n",
    "    #Get list of nodes from graph\n",
    "    nodeList =  testGraph.nodes.data()\n",
    "    #nodeList = list(networkx.dfs_preorder_nodes(testGraph, source='UAV'))\n",
    "    #print(nodeList)\n",
    "    #iterate through nodes\n",
    "    for node, values in nodeList:\n",
    "        #Get the value of each node\n",
    "        currentNodeValue = values['value']\n",
    "        #If the word we are looking for is in a node's value\n",
    "        if(wordToFind in currentNodeValue):\n",
    "            #print the node and its value\n",
    "            #print(node, currentNodeValue)\n",
    "            #Get the node's neighbors\n",
    "            neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "            #for each neighbor, append its value to the anchor node values\n",
    "            for neighbor in neighborNodes:\n",
    "                neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                anchorNodeValues.append(neighborNodeValue.lower())\n",
    "        else:\n",
    "            nodeValueList = currentNodeValue.split('|')\n",
    "            for value in nodeValueList:\n",
    "                anchorNodeValues.append(value)\n",
    "    #print(anchorNodeValues)\n",
    "    return anchorNodeValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPoSAnchors(checkWordList):\n",
    "    #Define vars\n",
    "    nounAnchorNodeValues = []\n",
    "    verbAnchorNodeValues = {}\n",
    "    #Get list of nodes from graph\n",
    "    nodeList =  testGraph.nodes.data()\n",
    "    #nodeList = list(networkx.dfs_preorder_nodes(testGraph, source='UAV'))\n",
    "    #print(nodeList)\n",
    "    #iterate through nodes\n",
    "    for wordToFind in checkWordList:\n",
    "        tag = checkWordList[wordToFind]\n",
    "        if(tag != 'VERB'):\n",
    "            for node, values in nodeList:\n",
    "                #Get the value of each node\n",
    "                currentNodeValue = values['value']\n",
    "                #If the word we are looking for is in a node's value\n",
    "                if(wordToFind in currentNodeValue):\n",
    "                    #print the node and its value\n",
    "                    #print(node, currentNodeValue)\n",
    "                    #Get the node's neighbors\n",
    "                    neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "                    #for each neighbor, append its value to the anchor node values\n",
    "                    for neighbor in neighborNodes:\n",
    "                        neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                        nounAnchorNodeValues.append(neighborNodeValue.lower())\n",
    "                else:\n",
    "                    nodeValueList = currentNodeValue.split('|')\n",
    "                    for value in nodeValueList:\n",
    "                        nounAnchorNodeValues.append(value)\n",
    "            if(tag == 'VERB'):\n",
    "                #Get the value of each node\n",
    "                currentNodeValue = values['value']\n",
    "                #If the word we are looking for is in a node's value\n",
    "                if(wordToFind in currentNodeValue):\n",
    "                    #print the node and its value\n",
    "                    #print(node, currentNodeValue)\n",
    "                    #Get the node's neighbors\n",
    "                    neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "                    #for each neighbor, append its value to the anchor node values\n",
    "                    for neighbor in neighborNodes:\n",
    "                        neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                        verbAnchorNodeValues.append({neighborNodeValue.lower(): wordToFind})\n",
    "                else:\n",
    "                    nodeValueList = currentNodeValue.split('|')\n",
    "                    for value in nodeValueList:\n",
    "                        verbAnchorNodeValues.append({value: wordToFind})\n",
    "    #print(anchorNodeValues)\n",
    "    return nounAnchorNodeValues, verbAnchorNodeValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltkPoSTag(partsOfInterest, wordList):\n",
    "    #FORCE 'MOVE' TO RETRUN AS A VERB - HARDCODED FOR TESTING BECAUSE TAGGER RETURNS NOUN\n",
    "    anchorWords = {}\n",
    "    checkWordList = {}\n",
    "    verbFound = False\n",
    "    #Tag each word with a part of speech (using the universal tagging system for more general tags)\n",
    "    tagged = pos_tag(wordList, tagset='universal', lang='eng')\n",
    "    #tagged = pos_tag(wordList)\n",
    "    #Go through each word and find if it's a part of speech we are interested in\n",
    "    for tagPair in tagged:\n",
    "        #get word and tag\n",
    "        word = tagPair[0]\n",
    "        tag = tagPair[1]\n",
    "        #If interesting part of speech, then append to list of words to check\n",
    "        print(word, tag)\n",
    "        if(word != 'move'):\n",
    "            if tag in set(partsOfInterest):\n",
    "                checkWordList.update({word: tag})\n",
    "                #if tag == 'VERB':\n",
    "                    #verbFound = True\n",
    "        #else:\n",
    "            #checkWordList.update({word: 'VERB'})\n",
    "            #verbFound = True\n",
    "\n",
    "    return checkWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "def spacyPoSTag(partsOfInterest, wordList):\n",
    "    sp = spacy.load('en')\n",
    "    #FORCE 'MOVE' TO RETRUN AS A VERB - HARDCODED FOR TESTING BECAUSE TAGGER RETURNS NOUN\n",
    "    #anchorWords = {}\n",
    "    checkWordList = {}\n",
    "    tagged = {}\n",
    "    #Tag each word with a part of speech (using the universal tagging system for more general tags)\n",
    "    for word in wordList:\n",
    "        tagged.update({word: word.pos_})\n",
    "        print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')    \n",
    "        #tagged = pos_tag(wordList)\n",
    "    #Go through each word and find if it's a part of speech we are interested in\n",
    "    print(tagged)\n",
    "    for word in tagged:\n",
    "        #get word and tag\n",
    "        tag = tagged[word]\n",
    "        #If interesting part of speech, then append to list of words to check\n",
    "        print(word, tag)\n",
    "        #if(word != 'move'):\n",
    "        if tag in set(partsOfInterest):\n",
    "            checkWordList.update({word.text: tag})\n",
    "        #    if tag == 'VERB':\n",
    "        #       verbFound = True\n",
    "        #else:\n",
    "        #   checkWordList.update({word: 'VERB'})\n",
    "        #  verbFound = True\n",
    "\n",
    "    return checkWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a command\n",
      "hi\n",
      "hi           INTJ       UH       interjection\n",
      "{hi: 'INTJ'}\n",
      "hi INTJ\n",
      "hi NOUN\n",
      "{}\n",
      "{'hi': 'NOUN'}\n",
      "hi\n",
      "hi\n",
      "dict_items([])\n",
      "You gave the following information: \n",
      "\n",
      "hi\n",
      "Is this equivalent to the following?\n",
      "\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from builtins import any as b_any\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "sp = spacy.load('en')\n",
    "\n",
    "#ignoredWords = ['the', 'a', 'at']\n",
    "\n",
    "#define the noun that we have that already exists\n",
    "meantWords = {}\n",
    "partsOfInterest = ['VERB', 'NOUN']\n",
    "nltkUniversalTagset = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']\n",
    "#Define the input command\n",
    "#inputCommand = \"Find the objective at the point with the target\"\n",
    "#inputCommand = \"There are no limits on airspeed or altitude\"\n",
    "#inputCommand = \"Bring me six green peppers from Meijer\"\n",
    "#inputCommand = \"The first place is LVN.  It is a target.  The airspeed constraint is 200 and the altitude limit is 1500.  The effective radius is 2.5\"\n",
    "#inputCommand = \"The first waypoint is LVN.  It is a target.  The airspeed restriction is 200 and the altitude restriction is 1500.  The effective radius is 2.5\"\n",
    "#inputCommand = \"What is our current airspeed?\"\n",
    "#inputCommand = \"Go to the next location\"\n",
    "#inputCommand = \"Go faster\"\n",
    "inputCommand = input(\"Please enter a command\\n\")\n",
    "inputCommand = inputCommand.lower()\n",
    "#Tokenize the input command\n",
    "nltkWordList = word_tokenize(inputCommand)\n",
    "spacyWordList = sp(inputCommand)\n",
    "checkWordList = {}\n",
    "tempCheckWordList = {}\n",
    "anchorWords = {}\n",
    "#PoSTag word list and get anchor words and important words to check back\n",
    "spacycheckWordList = spacyPoSTag(partsOfInterest, spacyWordList)\n",
    "nltkcheckWordList = nltkPoSTag(partsOfInterest, nltkWordList)\n",
    "\n",
    "print(spacycheckWordList)\n",
    "print(nltkcheckWordList)\n",
    "#HOW TO HANDLE DISAGREEMENTS?  SUPERSET OF NOUNS AND VERBS?  INTERANNOTATOR AGREEMENTS?\n",
    "#if(verbFound == False):\n",
    "\n",
    "#convert spacy tokens to strings\n",
    "#for word, tag in tempCheckWordList:\n",
    "#    word = word.text\n",
    "#    checkWordList.update({word: tag})\n",
    "\n",
    "for word in nltkcheckWordList:\n",
    "    print(word)\n",
    "    anchorsFound = findAnchors(word)\n",
    "    for anchor in anchorsFound:\n",
    "        #print(anchor)\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "'''else:\n",
    "    nounAnchorNodeValues, verbAnchorNodeValues = findPoSAnchors(checkWordList)\n",
    "    for anchor in nounAnchorNodeValues:\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "    for anchor, originalWord in verbAnchorNodeValues:\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "print (anchorWords)'''\n",
    "\n",
    "#Get syno/hyper/hyponyms and derivationally related forms\n",
    "synonyms, hypernyms, hyponyms, deriv = getNyms(anchorWords, nltkcheckWordList)\n",
    "\n",
    "#Iterate through all targetWord key-value pairs\n",
    "#for targetWord in anchorWords:\n",
    "for anchorWord in anchorWords:\n",
    "    endSearch = False\n",
    "    #print(anchorWord)\n",
    "    #print(anchorWords[anchorWord])\n",
    "    for originalWord in anchorWords[anchorWord]:\n",
    "        if(endSearch == False):\n",
    "            #Ignore if the found word is the target word itself\n",
    "            if(originalWord != anchorWord):\n",
    "                answered = False\n",
    "                #Check if one of the found words is correct\n",
    "                answer = input(\"When you said \" + originalWord + \", did you mean \" + anchorWord + \"?\\n\")\n",
    "                while(answered == False):\n",
    "                    if(answer == 'yes'):\n",
    "                        answered = True\n",
    "                        endSearch = True\n",
    "                        meantWords[originalWord] = anchorWord\n",
    "                    elif(answer == 'no'):\n",
    "                        answered = True\n",
    "                    else:\n",
    "                        answer = input(\"Please answer yes or no\\n\")\n",
    "                        answered = False\n",
    "\n",
    "#Set up a list for the words which were replaced                        \n",
    "print(meantWords.items())\n",
    "meantInput = []\n",
    "for word in nltkWordList:\n",
    "    meantInput.append(word)\n",
    "\n",
    "    \n",
    "#ADDED\n",
    "'''verbObjects = {}\n",
    "if (verbFound == True):\n",
    "    \n",
    "    for word in meantWords:\n",
    "        meantWord = meantWords[word]\n",
    "        checkWordList, anchorWords, verbFound = PoSTag(partsOfInterest, word_tokenize(meantWord))\n",
    "        if(checkWordList[meantWord] == 'VERB'):\n",
    "             #Get list of nodes from graph\n",
    "            nodeList =  testGraph.nodes.data()\n",
    "            #iterate through nodes\n",
    "            for node, values in nodeList:\n",
    "                #Get the value of each node\n",
    "                currentNodeValue = values['value']\n",
    "                #If the word we are looking for is in a node's value\n",
    "                if(meantWord in currentNodeValue):\n",
    "                    #Get the node's neighbors\n",
    "                    neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "                    #for each neighbor, append its value to the anchor node values\n",
    "                    for neighbor in neighborNodes:\n",
    "                        neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                        verbObjects.update({meantWord: neighborNodeValue.lower()})\n",
    "print (verbObjects)'''\n",
    "\n",
    "#Replace the initial words with the found words\n",
    "for index, word in enumerate(meantInput):\n",
    "    for meantWordKey, meantWordValue in meantWords.items():\n",
    "        #print(word, meantWordKey, meantWordValue)\n",
    "        if(word == meantWordKey):\n",
    "            meantInput[index] = meantWordValue\n",
    "\n",
    "#Put together a potential string that means the same and output it for confirmation\n",
    "meantInputString = ' '.join(meantInput)\n",
    "#if(verbFound == False):\n",
    "print(\"You gave the following information: \\n\")\n",
    "print(inputCommand)\n",
    "print(\"Is this equivalent to the following?\\n\")\n",
    "print(meantInputString)\n",
    "\n",
    "\n",
    "#VERY SHAKY WAY TO RECONSTRUCT SENTENCE\n",
    "#if(verbFound == True):\n",
    "#    print(\"You gave the following instruction: \\n\")\n",
    "#    print(inputCommand)\n",
    "#    verbAction, verbObject = verbObjects.popitem()\n",
    "#    print(\"Were you requesting for the \" + verbObject + \" to \" + meantInputString + \"?\\n\")\n",
    "#    print(meantInputString)\n",
    "\n",
    "    \n",
    "#correctSolution = input(\"Please answer yes or no\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cb3244504100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#sentence = sp(u\"There are no limits on airspeed or altitude over the objective\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#sentence = sp(\"Move faster\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minputCommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please enter a command\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0minputCommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Tokenize the input command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "sp = spacy.load('en')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#sentence = sp(u\"There are no limits on airspeed or altitude over the objective\")\n",
    "#sentence = sp(\"Move faster\")\n",
    "inputCommand = input(\"Please enter a command\\n\")\n",
    "inputCommand = inputCommand.lower()\n",
    "#Tokenize the input command\n",
    "#wordList = word_tokenize(inputCommand)\n",
    "sentence = sp(inputCommand)\n",
    "checkWordList = {}\n",
    "\n",
    "#PoSTag word list and get anchor words and important words to check back\n",
    "checkWordList, anchorWords, verbFound = PoSTag(sentence)\n",
    "#displacy.render(sentence, style='dep', jupyter=True, options={'distance': 85})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet, verbnet, framenet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "#import nltk \n",
    "#from nltk.tag.stanford import StanfordTagger\n",
    "#print(verbnet.classids(lemma='go'))\n",
    "#_path_to_model = 'E:/nltk_data/stanford-postagger-2015-04-20/models/english-bidirectional-distsim.tagger'\n",
    "#_path_to_jar = 'E:/nltk_data/stanford-postagger-2015-04-20/stanford-postagger.jar'\n",
    "#tagger = StanfordTagger(path_to_model=_path_to_model)\n",
    "goFrames = framenet.lus(r'^go.v\\b')\n",
    "partsOfInterest = ['VERB']\n",
    "for frame in goFrames:\n",
    "    definitionTokens = word_tokenize(frame.definition)\n",
    "    definitionTokens = definitionTokens[2:]\n",
    "    print(definitionTokens)\n",
    "    taggedDefinition = PoSTag(partsOfInterest, definitionTokens)\n",
    "    print(taggedDefinition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from builtins import any as b_any\n",
    "\n",
    "#ignoredWords = ['the', 'a', 'at']\n",
    "\n",
    "#define the noun that we have that already exists\n",
    "meantWords = {}\n",
    "partsOfInterest = ['VERB', 'NOUN']\n",
    "#Define the input command\n",
    "#inputCommand = \"Find the objective at the point with the target\"\n",
    "#inputCommand = \"There are no limits on airspeed or altitude\"\n",
    "#inputCommand = \"Bring me six green peppers from Meijer\"\n",
    "#inputCommand = \"The first place is LVN.  It is a target.  The airspeed constraint is 200 and the altitude limit is 1500.  The effective radius is 2.5\"\n",
    "#inputCommand = \"The first waypoint is LVN.  It is a target.  The airspeed restriction is 200 and the altitude restriction is 1500.  The effective radius is 2.5\"\n",
    "#inputCommand = \"What is our current airspeed?\"\n",
    "#inputCommand = \"Go to the next location\"\n",
    "#inputCommand = \"Go faster\"\n",
    "inputCommand = input(\"Please enter a command\\n\")\n",
    "inputCommand = inputCommand.lower()\n",
    "#Tokenize the input command\n",
    "wordList = word_tokenize(inputCommand)\n",
    "checkWordList = {}\n",
    "\n",
    "#PoSTag word list and get anchor words and important words to check back\n",
    "checkWordList, anchorWords, verbFound = PoSTag(partsOfInterest, wordList)\n",
    "\n",
    "#if(verbFound == False):\n",
    "for word in checkWordList:\n",
    "    anchorsFound = findAnchors(word)\n",
    "    for anchor in anchorsFound:\n",
    "        #print(anchor)\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "'''else:\n",
    "    nounAnchorNodeValues, verbAnchorNodeValues = findPoSAnchors(checkWordList)\n",
    "    for anchor in nounAnchorNodeValues:\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "    for anchor, originalWord in verbAnchorNodeValues:\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "print (anchorWords)'''\n",
    "\n",
    "#Get syno/hyper/hyponyms and derivationally related forms\n",
    "synonyms, hypernyms, hyponyms, deriv = getNyms(anchorWords, checkWordList)\n",
    "\n",
    "#Iterate through all targetWord key-value pairs\n",
    "#for targetWord in anchorWords:\n",
    "for anchorWord in anchorWords:\n",
    "    endSearch = False\n",
    "    #print(anchorWord)\n",
    "    #print(anchorWords[anchorWord])\n",
    "    for originalWord in anchorWords[anchorWord]:\n",
    "        if(endSearch == False):\n",
    "            #Ignore if the found word is the target word itself\n",
    "            if(originalWord != anchorWord):\n",
    "                answered = False\n",
    "                #Check if one of the found words is correct\n",
    "                answer = input(\"When you said \" + originalWord + \", did you mean \" + anchorWord + \"?\\n\")\n",
    "                while(answered == False):\n",
    "                    if(answer == 'yes'):\n",
    "                        answered = True\n",
    "                        endSearch = True\n",
    "                        meantWords[originalWord] = anchorWord\n",
    "                    elif(answer == 'no'):\n",
    "                        answered = True\n",
    "                    else:\n",
    "                        answer = input(\"Please answer yes or no\\n\")\n",
    "                        answered = False\n",
    "\n",
    "#Set up a list for the words which were replaced                        \n",
    "print(meantWords.items())\n",
    "meantInput = []\n",
    "for word in wordList:\n",
    "    meantInput.append(word)\n",
    "\n",
    "    \n",
    "#ADDED\n",
    "'''verbObjects = {}\n",
    "if (verbFound == True):\n",
    "    \n",
    "    for word in meantWords:\n",
    "        meantWord = meantWords[word]\n",
    "        checkWordList, anchorWords, verbFound = PoSTag(partsOfInterest, word_tokenize(meantWord))\n",
    "        if(checkWordList[meantWord] == 'VERB'):\n",
    "             #Get list of nodes from graph\n",
    "            nodeList =  testGraph.nodes.data()\n",
    "            #iterate through nodes\n",
    "            for node, values in nodeList:\n",
    "                #Get the value of each node\n",
    "                currentNodeValue = values['value']\n",
    "                #If the word we are looking for is in a node's value\n",
    "                if(meantWord in currentNodeValue):\n",
    "                    #Get the node's neighbors\n",
    "                    neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "                    #for each neighbor, append its value to the anchor node values\n",
    "                    for neighbor in neighborNodes:\n",
    "                        neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                        verbObjects.update({meantWord: neighborNodeValue.lower()})\n",
    "print (verbObjects)'''\n",
    "\n",
    "#Replace the initial words with the found words\n",
    "for index, word in enumerate(meantInput):\n",
    "    for meantWordKey, meantWordValue in meantWords.items():\n",
    "        #print(word, meantWordKey, meantWordValue)\n",
    "        if(word == meantWordKey):\n",
    "            meantInput[index] = meantWordValue\n",
    "\n",
    "#Put together a potential string that means the same and output it for confirmation\n",
    "meantInputString = ' '.join(meantInput)\n",
    "#if(verbFound == False):\n",
    "print(\"You gave the following information: \\n\")\n",
    "print(inputCommand)\n",
    "print(\"Is this equivalent to the following?\\n\")\n",
    "print(meantInputString)\n",
    "\n",
    "'''\n",
    "#VERY SHAKY WAY TO RECONSTRUCT SENTENCE\n",
    "if(verbFound == True):\n",
    "    print(\"You gave the following instruction: \\n\")\n",
    "    print(inputCommand)\n",
    "    verbAction, verbObject = verbObjects.popitem()\n",
    "    print(\"Were you requesting for the \" + verbObject + \" to \" + meantInputString + \"?\\n\")\n",
    "    print(meantInputString)\n",
    "'''\n",
    "    \n",
    "#correctSolution = input(\"Please answer yes or no\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
