{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if not cb.iterable(width):\n",
      "C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:676: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if cb.iterable(node_size):  # many node sizes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "#Create Graph\n",
    "testGraph = networkx.MultiDiGraph()\n",
    "testGraph.add_node('UAV', value = 'UAV')\n",
    "testGraph.add_node('UAVAttributeType', value = 'course|speed|altitude|flaps|gear|fuel')\n",
    "testGraph.add_node('UAVAttributeValue', value = '<number>|unknown')\n",
    "testGraph.add_node('UAVAttributeStatus', value = 'up|down|unknown')\n",
    "testGraph.add_node('UAVAttributeAlarm', value = 'ok|warning|alarm')\n",
    "testGraph.add_node('UAVAttributeStable', value = 'true|false|unknown')\n",
    "testGraph.add_node('UAVAttributeChange', value = 'increase|decrease|stop')\n",
    "testGraph.add_node('UAVAttributeAction', value = 'move|climb|descend|speed_up|slow_down')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeType', value = 'UAVhasType')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeValue', value = 'UAVhasValue')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeStatus', value = 'UAVhasStatus')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeAlarm', value = 'UAVhasAlarm')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeStable', value = 'UAVhasStable')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeChange', value = 'UAVhasChange')\n",
    "testGraph.add_edge('UAV', 'UAVAttributeAction', value = 'UAVhasAction')\n",
    "\n",
    "testGraph.add_node('Restriction', value='Restriction')\n",
    "testGraph.add_node('RestrictionUpper', value='<number>|none|unknown')\n",
    "testGraph.add_node('RestrictionLower', value='<number>|none|unknown')\n",
    "testGraph.add_node('RestrictionType', value='altitude|speed|photo')\n",
    "testGraph.add_edge('Restriction', 'RestrictionUpper', value='RestrictionHasUpper')\n",
    "testGraph.add_edge('Restriction', 'RestrictionLower', value='RestrictionHasLower')\n",
    "testGraph.add_edge('Restriction', 'RestrictionType', value='RestrictionHasType')\n",
    "\n",
    "testGraph.add_edge('UAV', 'Restriction', value='hasRestriction')\n",
    "\n",
    "#Draw Graph\n",
    "networkx.draw(testGraph, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNyms(anchorWords, checkWordList):\n",
    "    #Iterate through all words to check\n",
    "    synonyms = []\n",
    "    hypernyms = []\n",
    "    hyponyms = []\n",
    "    deriv = []\n",
    "    for currentWord in checkWordList:\n",
    "        synonyms.clear()\n",
    "        hypernyms.clear()\n",
    "        hyponyms.clear()\n",
    "        deriv.clear()\n",
    "        print(currentWord)\n",
    "        #print(currentWord)\n",
    "        #print(checkWordList[currentWord])\n",
    "        #Get synsets of current word to check\n",
    "        testWord = wordnet.synsets(currentWord)\n",
    "        #for each synset (meaning)\n",
    "        for syn in testWord:\n",
    "            #Get Hypernyms\n",
    "            if(len(syn.hypernyms()) > 0):\n",
    "                currentHypernyms = syn.hypernyms()\n",
    "                for hyperSyn in currentHypernyms:\n",
    "                    for lemma in hyperSyn.lemmas():\n",
    "                        #if(lemma.name() != currentWord):\n",
    "                        hypernyms.append(lemma.name())\n",
    "                    #hypernyms.append(hyperSyn.lemma_names())\n",
    "            #Get Hyponyms\n",
    "            if(len(syn.hyponyms()) > 0):\n",
    "                currentHyponyms = syn.hyponyms()\n",
    "                for hypoSyn in currentHyponyms:\n",
    "                    for lemma in hypoSyn.lemmas():\n",
    "                        #if(lemma.name() != currentWord):\n",
    "                        hyponyms.append(lemma.name())\n",
    "                    #hypernyms.append(hyperSyn.lemma_names())\n",
    "            #Get direct synonyms\n",
    "            for lemma in syn.lemmas():\n",
    "                #if(lemma.name() != currentWord):\n",
    "                synonyms.append(lemma.name())\n",
    "                #Get derivationally related forms\n",
    "                for derivForm in lemma.derivationally_related_forms():\n",
    "                    if(derivForm.name() not in deriv):\n",
    "                        deriv.append(derivForm.name())\n",
    "        #print(\"SYNONYMS: \")\n",
    "        #print(set(synonyms))\n",
    "        #print('\\n HYPERNYMS:')\n",
    "        #print(set(hypernyms))\n",
    "        #print('\\n HYPONYMS:')\n",
    "        #print(set(hyponyms))\n",
    "        #print('\\n DERIVATIONALLY RELATED FORMS:')\n",
    "        #print(set(deriv))\n",
    "        #Check if any target words found in syno/hyper/hypo lists\n",
    "        #If target word is found, increase the number of times found in the dictionary.\n",
    "        for targetWord in anchorWords:\n",
    "            if targetWord in set(synonyms):\n",
    "                anchorWords[targetWord].append(currentWord)\n",
    "            elif targetWord in set(hypernyms):\n",
    "                anchorWords[targetWord].append(currentWord)\n",
    "            elif targetWord in set(hyponyms):\n",
    "                anchorWords[targetWord].append(currentWord)\n",
    "            elif targetWord in set(deriv):\n",
    "                anchorWords[targetWord].append(currentWord)\n",
    "    return (synonyms, hypernyms, hyponyms, deriv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAnchors(wordToFind):\n",
    "    #Define vars\n",
    "    anchorNodeValues = []\n",
    "    #Get list of nodes from graph\n",
    "    nodeList =  testGraph.nodes.data()\n",
    "    #nodeList = list(networkx.dfs_preorder_nodes(testGraph, source='UAV'))\n",
    "    #print(nodeList)\n",
    "    #iterate through nodes\n",
    "    for node, values in nodeList:\n",
    "        #Get the value of each node\n",
    "        currentNodeValue = values['value']\n",
    "        #If the word we are looking for is in a node's value\n",
    "        if(wordToFind in currentNodeValue):\n",
    "            #print the node and its value\n",
    "            #print(node, currentNodeValue)\n",
    "            #Get the node's neighbors\n",
    "            neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "            #for each neighbor, append its value to the anchor node values\n",
    "            for neighbor in neighborNodes:\n",
    "                neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                anchorNodeValues.append(neighborNodeValue.lower())\n",
    "        else:\n",
    "            nodeValueList = currentNodeValue.split('|')\n",
    "            for value in nodeValueList:\n",
    "                anchorNodeValues.append(value)\n",
    "    #print(anchorNodeValues)\n",
    "    return anchorNodeValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPoSAnchors(checkWordList):\n",
    "    #Define vars\n",
    "    nounAnchorNodeValues = []\n",
    "    verbAnchorNodeValues = {}\n",
    "    #Get list of nodes from graph\n",
    "    nodeList =  testGraph.nodes.data()\n",
    "    #nodeList = list(networkx.dfs_preorder_nodes(testGraph, source='UAV'))\n",
    "    #print(nodeList)\n",
    "    #iterate through nodes\n",
    "    for wordToFind in checkWordList:\n",
    "        tag = checkWordList[wordToFind]\n",
    "        if(tag != 'VERB'):\n",
    "            for node, values in nodeList:\n",
    "                #Get the value of each node\n",
    "                currentNodeValue = values['value']\n",
    "                #If the word we are looking for is in a node's value\n",
    "                if(wordToFind in currentNodeValue):\n",
    "                    #print the node and its value\n",
    "                    #print(node, currentNodeValue)\n",
    "                    #Get the node's neighbors\n",
    "                    neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "                    #for each neighbor, append its value to the anchor node values\n",
    "                    for neighbor in neighborNodes:\n",
    "                        neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                        nounAnchorNodeValues.append(neighborNodeValue.lower())\n",
    "                else:\n",
    "                    nodeValueList = currentNodeValue.split('|')\n",
    "                    for value in nodeValueList:\n",
    "                        nounAnchorNodeValues.append(value)\n",
    "            if(tag == 'VERB'):\n",
    "                #Get the value of each node\n",
    "                currentNodeValue = values['value']\n",
    "                #If the word we are looking for is in a node's value\n",
    "                if(wordToFind in currentNodeValue):\n",
    "                    #print the node and its value\n",
    "                    #print(node, currentNodeValue)\n",
    "                    #Get the node's neighbors\n",
    "                    neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "                    #for each neighbor, append its value to the anchor node values\n",
    "                    for neighbor in neighborNodes:\n",
    "                        neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                        verbAnchorNodeValues.append({neighborNodeValue.lower(): wordToFind})\n",
    "                else:\n",
    "                    nodeValueList = currentNodeValue.split('|')\n",
    "                    for value in nodeValueList:\n",
    "                        verbAnchorNodeValues.append({value: wordToFind})\n",
    "    #print(anchorNodeValues)\n",
    "    return nounAnchorNodeValues, verbAnchorNodeValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltkPoSTag(partsOfInterest, wordList):\n",
    "    #FORCE 'MOVE' TO RETRUN AS A VERB - HARDCODED FOR TESTING BECAUSE TAGGER RETURNS NOUN\n",
    "    anchorWords = {}\n",
    "    checkWordList = {}\n",
    "    verbFound = False\n",
    "    #Tag each word with a part of speech (using the universal tagging system for more general tags)\n",
    "    tagged = pos_tag(wordList, tagset='universal', lang='eng')\n",
    "    #tagged = pos_tag(wordList)\n",
    "    #Go through each word and find if it's a part of speech we are interested in\n",
    "    for tagPair in tagged:\n",
    "        #get word and tag\n",
    "        word = tagPair[0]\n",
    "        tag = tagPair[1]\n",
    "        #If interesting part of speech, then append to list of words to check\n",
    "        print(word, tag)\n",
    "        if(word != 'move'):\n",
    "            if tag in set(partsOfInterest):\n",
    "                checkWordList.update({word: tag})\n",
    "                #if tag == 'VERB':\n",
    "                    #verbFound = True\n",
    "        #else:\n",
    "            #checkWordList.update({word: 'VERB'})\n",
    "            #verbFound = True\n",
    "\n",
    "    return checkWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "def spacyPoSTag(partsOfInterest, wordList):\n",
    "    sp = spacy.load('en')\n",
    "    #FORCE 'MOVE' TO RETRUN AS A VERB - HARDCODED FOR TESTING BECAUSE TAGGER RETURNS NOUN\n",
    "    #anchorWords = {}\n",
    "    checkWordList = {}\n",
    "    tagged = {}\n",
    "    #Tag each word with a part of speech (using the universal tagging system for more general tags)\n",
    "    for word in wordList:\n",
    "        tagged.update({word: word.pos_})\n",
    "        print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')    \n",
    "        #tagged = pos_tag(wordList)\n",
    "    #Go through each word and find if it's a part of speech we are interested in\n",
    "    print(tagged)\n",
    "    for word in tagged:\n",
    "        #get word and tag\n",
    "        tag = tagged[word]\n",
    "        #If interesting part of speech, then append to list of words to check\n",
    "        print(word, tag)\n",
    "        #if(word != 'move'):\n",
    "        if tag in set(partsOfInterest):\n",
    "            checkWordList.update({word.text: tag})\n",
    "        #    if tag == 'VERB':\n",
    "        #       verbFound = True\n",
    "        #else:\n",
    "        #   checkWordList.update({word: 'VERB'})\n",
    "        #  verbFound = True\n",
    "\n",
    "    return checkWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a command\n",
      "There is no limit on airspeed or altitude.\n",
      "there        ADV        EX       existential there\n",
      "is           VERB       VBZ      verb, 3rd person singular present\n",
      "no           DET        DT       determiner\n",
      "limit        NOUN       NN       noun, singular or mass\n",
      "on           ADP        IN       conjunction, subordinating or preposition\n",
      "airspeed     NUM        CD       cardinal number\n",
      "or           CCONJ      CC       conjunction, coordinating\n",
      "altitude     NOUN       NN       noun, singular or mass\n",
      ".            PUNCT      .        punctuation mark, sentence closer\n",
      "{there: 'ADV', is: 'VERB', no: 'DET', limit: 'NOUN', on: 'ADP', airspeed: 'NUM', or: 'CCONJ', altitude: 'NOUN', .: 'PUNCT'}\n",
      "there ADV\n",
      "is VERB\n",
      "no DET\n",
      "limit NOUN\n",
      "on ADP\n",
      "airspeed NUM\n",
      "or CCONJ\n",
      "altitude NOUN\n",
      ". PUNCT\n",
      "there DET\n",
      "is VERB\n",
      "no DET\n",
      "limit NOUN\n",
      "on ADP\n",
      "airspeed NOUN\n",
      "or CONJ\n",
      "altitude NOUN\n",
      ". .\n",
      "{'is': 'VERB', 'limit': 'NOUN', 'altitude': 'NOUN'}\n",
      "{'is': 'VERB', 'limit': 'NOUN', 'airspeed': 'NOUN', 'altitude': 'NOUN'}\n",
      "is\n",
      "limit\n",
      "airspeed\n",
      "altitude\n",
      "is\n",
      "limit\n",
      "airspeed\n",
      "altitude\n",
      "When you said airspeed, did you mean speed?\n",
      "yes\n",
      "When you said limit, did you mean decrease?\n",
      "no\n",
      "When you said limit, did you mean restriction?\n",
      "yes\n",
      "dict_items([('airspeed', 'speed'), ('limit', 'restriction')])\n",
      "You gave the following information: \n",
      "\n",
      "there is no limit on airspeed or altitude.\n",
      "Is this equivalent to the following?\n",
      "\n",
      "there is no restriction on speed or altitude .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from builtins import any as b_any\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "sp = spacy.load('en')\n",
    "\n",
    "#ignoredWords = ['the', 'a', 'at']\n",
    "\n",
    "#define the noun that we have that already exists\n",
    "meantWords = {}\n",
    "partsOfInterest = ['VERB', 'NOUN']\n",
    "nltkUniversalTagset = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']\n",
    "#Define the input command\n",
    "#inputCommand = \"Find the objective at the point with the target\"\n",
    "#inputCommand = \"There are no limits on airspeed or altitude\"\n",
    "#inputCommand = \"Bring me six green peppers from Meijer\"\n",
    "#inputCommand = \"The first place is LVN.  It is a target.  The airspeed constraint is 200 and the altitude limit is 1500.  The effective radius is 2.5\"\n",
    "#inputCommand = \"The first waypoint is LVN.  It is a target.  The airspeed restriction is 200 and the altitude restriction is 1500.  The effective radius is 2.5\"\n",
    "#inputCommand = \"What is our current airspeed?\"\n",
    "#inputCommand = \"Go to the next location\"\n",
    "#inputCommand = \"Go faster\"\n",
    "inputCommand = input(\"Please enter a command\\n\")\n",
    "inputCommand = inputCommand.lower()\n",
    "#Tokenize the input command\n",
    "nltkWordList = word_tokenize(inputCommand)\n",
    "spacyWordList = sp(inputCommand)\n",
    "checkWordList = {}\n",
    "tempCheckWordList = {}\n",
    "anchorWords = {}\n",
    "#PoSTag word list and get anchor words and important words to check back\n",
    "spacycheckWordList = spacyPoSTag(partsOfInterest, spacyWordList)\n",
    "nltkcheckWordList = nltkPoSTag(partsOfInterest, nltkWordList)\n",
    "\n",
    "print(spacycheckWordList)\n",
    "print(nltkcheckWordList)\n",
    "#HOW TO HANDLE DISAGREEMENTS?  SUPERSET OF NOUNS AND VERBS?  INTERANNOTATOR AGREEMENTS?\n",
    "#if(verbFound == False):\n",
    "\n",
    "#convert spacy tokens to strings\n",
    "#for word, tag in tempCheckWordList:\n",
    "#    word = word.text\n",
    "#    checkWordList.update({word: tag})\n",
    "\n",
    "for word in nltkcheckWordList:\n",
    "    print(word)\n",
    "    anchorsFound = findAnchors(word)\n",
    "    for anchor in anchorsFound:\n",
    "        #print(anchor)\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "'''else:\n",
    "    nounAnchorNodeValues, verbAnchorNodeValues = findPoSAnchors(checkWordList)\n",
    "    for anchor in nounAnchorNodeValues:\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "    for anchor, originalWord in verbAnchorNodeValues:\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "print (anchorWords)'''\n",
    "\n",
    "#Get syno/hyper/hyponyms and derivationally related forms\n",
    "synonyms, hypernyms, hyponyms, deriv = getNyms(anchorWords, nltkcheckWordList)\n",
    "\n",
    "#Iterate through all targetWord key-value pairs\n",
    "#for targetWord in anchorWords:\n",
    "for anchorWord in anchorWords:\n",
    "    endSearch = False\n",
    "    #print(anchorWord)\n",
    "    #print(anchorWords[anchorWord])\n",
    "    for originalWord in anchorWords[anchorWord]:\n",
    "        if(endSearch == False):\n",
    "            #Ignore if the found word is the target word itself\n",
    "            if(originalWord != anchorWord):\n",
    "                answered = False\n",
    "                #Check if one of the found words is correct\n",
    "                answer = input(\"When you said \" + originalWord + \", did you mean \" + anchorWord + \"?\\n\")\n",
    "                while(answered == False):\n",
    "                    if(answer == 'yes'):\n",
    "                        answered = True\n",
    "                        endSearch = True\n",
    "                        meantWords[originalWord] = anchorWord\n",
    "                    elif(answer == 'no'):\n",
    "                        answered = True\n",
    "                    else:\n",
    "                        answer = input(\"Please answer yes or no\\n\")\n",
    "                        answered = False\n",
    "\n",
    "#Set up a list for the words which were replaced                        \n",
    "print(meantWords.items())\n",
    "meantInput = []\n",
    "for word in nltkWordList:\n",
    "    meantInput.append(word)\n",
    "\n",
    "    \n",
    "#ADDED\n",
    "'''verbObjects = {}\n",
    "if (verbFound == True):\n",
    "    \n",
    "    for word in meantWords:\n",
    "        meantWord = meantWords[word]\n",
    "        checkWordList, anchorWords, verbFound = PoSTag(partsOfInterest, word_tokenize(meantWord))\n",
    "        if(checkWordList[meantWord] == 'VERB'):\n",
    "             #Get list of nodes from graph\n",
    "            nodeList =  testGraph.nodes.data()\n",
    "            #iterate through nodes\n",
    "            for node, values in nodeList:\n",
    "                #Get the value of each node\n",
    "                currentNodeValue = values['value']\n",
    "                #If the word we are looking for is in a node's value\n",
    "                if(meantWord in currentNodeValue):\n",
    "                    #Get the node's neighbors\n",
    "                    neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "                    #for each neighbor, append its value to the anchor node values\n",
    "                    for neighbor in neighborNodes:\n",
    "                        neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                        verbObjects.update({meantWord: neighborNodeValue.lower()})\n",
    "print (verbObjects)'''\n",
    "\n",
    "#Replace the initial words with the found words\n",
    "for index, word in enumerate(meantInput):\n",
    "    for meantWordKey, meantWordValue in meantWords.items():\n",
    "        #print(word, meantWordKey, meantWordValue)\n",
    "        if(word == meantWordKey):\n",
    "            meantInput[index] = meantWordValue\n",
    "\n",
    "#Put together a potential string that means the same and output it for confirmation\n",
    "meantInputString = ' '.join(meantInput)\n",
    "#if(verbFound == False):\n",
    "print(\"You gave the following information: \\n\")\n",
    "print(inputCommand)\n",
    "print(\"Is this equivalent to the following?\\n\")\n",
    "print(meantInputString)\n",
    "\n",
    "\n",
    "#VERY SHAKY WAY TO RECONSTRUCT SENTENCE\n",
    "#if(verbFound == True):\n",
    "#    print(\"You gave the following instruction: \\n\")\n",
    "#    print(inputCommand)\n",
    "#    verbAction, verbObject = verbObjects.popitem()\n",
    "#    print(\"Were you requesting for the \" + verbObject + \" to \" + meantInputString + \"?\\n\")\n",
    "#    print(meantInputString)\n",
    "\n",
    "    \n",
    "#correctSolution = input(\"Please answer yes or no\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "sp = spacy.load('en')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#sentence = sp(u\"There are no limits on airspeed or altitude over the objective\")\n",
    "#sentence = sp(\"Move faster\")\n",
    "inputCommand = input(\"Please enter a command\\n\")\n",
    "inputCommand = inputCommand.lower()\n",
    "#Tokenize the input command\n",
    "#wordList = word_tokenize(inputCommand)\n",
    "sentence = sp(inputCommand)\n",
    "checkWordList = {}\n",
    "\n",
    "#PoSTag word list and get anchor words and important words to check back\n",
    "checkWordList, anchorWords, verbFound = PoSTag(sentence)\n",
    "#displacy.render(sentence, style='dep', jupyter=True, options={'distance': 85})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet, verbnet, framenet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "#import nltk \n",
    "#from nltk.tag.stanford import StanfordTagger\n",
    "#print(verbnet.classids(lemma='go'))\n",
    "#_path_to_model = 'E:/nltk_data/stanford-postagger-2015-04-20/models/english-bidirectional-distsim.tagger'\n",
    "#_path_to_jar = 'E:/nltk_data/stanford-postagger-2015-04-20/stanford-postagger.jar'\n",
    "#tagger = StanfordTagger(path_to_model=_path_to_model)\n",
    "goFrames = framenet.lus(r'^go.v\\b')\n",
    "partsOfInterest = ['VERB']\n",
    "for frame in goFrames:\n",
    "    definitionTokens = word_tokenize(frame.definition)\n",
    "    definitionTokens = definitionTokens[2:]\n",
    "    print(definitionTokens)\n",
    "    taggedDefinition = PoSTag(partsOfInterest, definitionTokens)\n",
    "    print(taggedDefinition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from builtins import any as b_any\n",
    "\n",
    "#ignoredWords = ['the', 'a', 'at']\n",
    "\n",
    "#define the noun that we have that already exists\n",
    "meantWords = {}\n",
    "partsOfInterest = ['VERB', 'NOUN']\n",
    "#Define the input command\n",
    "#inputCommand = \"Find the objective at the point with the target\"\n",
    "#inputCommand = \"There are no limits on airspeed or altitude\"\n",
    "#inputCommand = \"Bring me six green peppers from Meijer\"\n",
    "#inputCommand = \"The first place is LVN.  It is a target.  The airspeed constraint is 200 and the altitude limit is 1500.  The effective radius is 2.5\"\n",
    "#inputCommand = \"The first waypoint is LVN.  It is a target.  The airspeed restriction is 200 and the altitude restriction is 1500.  The effective radius is 2.5\"\n",
    "#inputCommand = \"What is our current airspeed?\"\n",
    "#inputCommand = \"Go to the next location\"\n",
    "#inputCommand = \"Go faster\"\n",
    "inputCommand = input(\"Please enter a command\\n\")\n",
    "inputCommand = inputCommand.lower()\n",
    "#Tokenize the input command\n",
    "wordList = word_tokenize(inputCommand)\n",
    "checkWordList = {}\n",
    "\n",
    "#PoSTag word list and get anchor words and important words to check back\n",
    "checkWordList, anchorWords, verbFound = PoSTag(partsOfInterest, wordList)\n",
    "\n",
    "#if(verbFound == False):\n",
    "for word in checkWordList:\n",
    "    anchorsFound = findAnchors(word)\n",
    "    for anchor in anchorsFound:\n",
    "        #print(anchor)\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "'''else:\n",
    "    nounAnchorNodeValues, verbAnchorNodeValues = findPoSAnchors(checkWordList)\n",
    "    for anchor in nounAnchorNodeValues:\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "    for anchor, originalWord in verbAnchorNodeValues:\n",
    "        if anchor not in anchorWords:\n",
    "            anchorWords[anchor] = []\n",
    "print (anchorWords)'''\n",
    "\n",
    "#Get syno/hyper/hyponyms and derivationally related forms\n",
    "synonyms, hypernyms, hyponyms, deriv = getNyms(anchorWords, checkWordList)\n",
    "\n",
    "#Iterate through all targetWord key-value pairs\n",
    "#for targetWord in anchorWords:\n",
    "for anchorWord in anchorWords:\n",
    "    endSearch = False\n",
    "    #print(anchorWord)\n",
    "    #print(anchorWords[anchorWord])\n",
    "    for originalWord in anchorWords[anchorWord]:\n",
    "        if(endSearch == False):\n",
    "            #Ignore if the found word is the target word itself\n",
    "            if(originalWord != anchorWord):\n",
    "                answered = False\n",
    "                #Check if one of the found words is correct\n",
    "                answer = input(\"When you said \" + originalWord + \", did you mean \" + anchorWord + \"?\\n\")\n",
    "                while(answered == False):\n",
    "                    if(answer == 'yes'):\n",
    "                        answered = True\n",
    "                        endSearch = True\n",
    "                        meantWords[originalWord] = anchorWord\n",
    "                    elif(answer == 'no'):\n",
    "                        answered = True\n",
    "                    else:\n",
    "                        answer = input(\"Please answer yes or no\\n\")\n",
    "                        answered = False\n",
    "\n",
    "#Set up a list for the words which were replaced                        \n",
    "print(meantWords.items())\n",
    "meantInput = []\n",
    "for word in wordList:\n",
    "    meantInput.append(word)\n",
    "\n",
    "    \n",
    "#ADDED\n",
    "'''verbObjects = {}\n",
    "if (verbFound == True):\n",
    "    \n",
    "    for word in meantWords:\n",
    "        meantWord = meantWords[word]\n",
    "        checkWordList, anchorWords, verbFound = PoSTag(partsOfInterest, word_tokenize(meantWord))\n",
    "        if(checkWordList[meantWord] == 'VERB'):\n",
    "             #Get list of nodes from graph\n",
    "            nodeList =  testGraph.nodes.data()\n",
    "            #iterate through nodes\n",
    "            for node, values in nodeList:\n",
    "                #Get the value of each node\n",
    "                currentNodeValue = values['value']\n",
    "                #If the word we are looking for is in a node's value\n",
    "                if(meantWord in currentNodeValue):\n",
    "                    #Get the node's neighbors\n",
    "                    neighborNodes = list(networkx.all_neighbors(testGraph, node))\n",
    "                    #for each neighbor, append its value to the anchor node values\n",
    "                    for neighbor in neighborNodes:\n",
    "                        neighborNodeValue = testGraph.node[neighbor]['value']\n",
    "                        verbObjects.update({meantWord: neighborNodeValue.lower()})\n",
    "print (verbObjects)'''\n",
    "\n",
    "#Replace the initial words with the found words\n",
    "for index, word in enumerate(meantInput):\n",
    "    for meantWordKey, meantWordValue in meantWords.items():\n",
    "        #print(word, meantWordKey, meantWordValue)\n",
    "        if(word == meantWordKey):\n",
    "            meantInput[index] = meantWordValue\n",
    "\n",
    "#Put together a potential string that means the same and output it for confirmation\n",
    "meantInputString = ' '.join(meantInput)\n",
    "#if(verbFound == False):\n",
    "print(\"You gave the following information: \\n\")\n",
    "print(inputCommand)\n",
    "print(\"Is this equivalent to the following?\\n\")\n",
    "print(meantInputString)\n",
    "\n",
    "'''\n",
    "#VERY SHAKY WAY TO RECONSTRUCT SENTENCE\n",
    "if(verbFound == True):\n",
    "    print(\"You gave the following instruction: \\n\")\n",
    "    print(inputCommand)\n",
    "    verbAction, verbObject = verbObjects.popitem()\n",
    "    print(\"Were you requesting for the \" + verbObject + \" to \" + meantInputString + \"?\\n\")\n",
    "    print(meantInputString)\n",
    "'''\n",
    "    \n",
    "#correctSolution = input(\"Please answer yes or no\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
